use probe_code::search::elastic_query;
use probe_code::search::tokenization;
// No term_exceptions import needed
use lru::LruCache;
use std::collections::{HashMap, HashSet};
use std::num::NonZeroUsize;
use std::sync::{Arc, Mutex};
use std::time::Instant;

/// Escapes special regex characters in a string
pub fn regex_escape(s: &str) -> String {
    let special_chars = [
        '.', '^', '$', '*', '+', '?', '(', ')', '[', ']', '{', '}', '|', '\\',
    ];
    let mut result = String::with_capacity(s.len() * 2);

    for c in s.chars() {
        if special_chars.contains(&c) {
            result.push('\\');
        }
        result.push(c);
    }

    result
}

// ----------------------------------------------------------------------------
// NEW CODE: Full AST-based planning and pattern generation
// ----------------------------------------------------------------------------

/// A unified plan holding the parsed AST and a mapping of each AST term to an index.
/// We store a map for quick lookups of term indices.
pub struct QueryPlan {
    pub ast: elastic_query::Expr,
    pub term_indices: HashMap<String, usize>,
    pub excluded_terms: HashSet<String>,
    pub exact: bool,
    /// Optimization hint: true if this is a simple single-term query
    pub is_simple_query: bool,
    /// Optimization hint: set of required terms that must all be present
    pub required_terms: HashSet<String>,

    // PHASE 3C OPTIMIZATION: Pre-computed AST metadata
    /// Pre-computed: whether the AST has any required term anywhere
    pub has_required_anywhere: bool,
    /// Pre-computed: indices of required terms for fast lookup
    pub required_terms_indices: HashSet<usize>,
    /// Pre-computed: whether AST has only excluded terms
    pub has_only_excluded_terms: bool,
    /// Evaluation result cache for matched term patterns
    pub evaluation_cache: Arc<Mutex<LruCache<u64, bool>>>,
    /// Flag indicating this is a universal query that should match all content
    /// (typically used when only filename filters are specified)
    pub is_universal_query: bool,

    // PHASE 5 OPTIMIZATION: Pre-computed special case terms
    /// Pre-computed: set of term indices that are special cases
    /// This avoids repeated is_special_case() calls during filtering
    pub special_case_indices: HashSet<usize>,
    /// Pre-computed: lowercase versions of special case terms for O(1) lookup
    pub special_case_terms_lower: HashMap<usize, String>,
}

impl std::fmt::Debug for QueryPlan {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        f.debug_struct("QueryPlan")
            .field("ast", &self.ast)
            .field("term_indices", &self.term_indices)
            .field("excluded_terms", &self.excluded_terms)
            .field("exact", &self.exact)
            .field("is_simple_query", &self.is_simple_query)
            .field("required_terms", &self.required_terms)
            .field("has_required_anywhere", &self.has_required_anywhere)
            .field("required_terms_indices", &self.required_terms_indices)
            .field("has_only_excluded_terms", &self.has_only_excluded_terms)
            .field("is_universal_query", &self.is_universal_query)
            .field("special_case_indices", &self.special_case_indices)
            .field("evaluation_cache", &"<LruCache>")
            .finish()
    }
}

/// Helper function to format duration in a human-readable way
fn format_duration(duration: std::time::Duration) -> String {
    if duration.as_millis() < 1000 {
        format!("{millis}ms", millis = duration.as_millis())
    } else {
        format!("{:.2}s", duration.as_secs_f64())
    }
}

/// Create a QueryPlan from a raw query string. This fully parses the query into an AST,
/// then extracts all terms (including excluded), and prepares a term-index map.
pub fn create_query_plan(query: &str, exact: bool) -> Result<QueryPlan, elastic_query::ParseError> {
    let debug_mode = std::env::var("DEBUG").unwrap_or_default() == "1";
    let start_time = Instant::now();

    if debug_mode {
        println!("DEBUG: Starting query plan creation for query: '{query}'");
    }

    // Use the regular AST parsing
    let parsing_start = Instant::now();

    if debug_mode {
        println!("DEBUG: Starting AST parsing for query: '{query}', exact={exact}");
    }

    // Parse the query into an AST with processed terms
    // We use standard Elasticsearch behavior (AND for implicit combinations)
    let mut ast = elastic_query::parse_query(query, exact)?;

    // If exact search is enabled, update the AST to mark all terms as exact
    if exact {
        update_ast_exact(&mut ast);
    }

    let parsing_duration = parsing_start.elapsed();

    if debug_mode {
        println!(
            "DEBUG: AST parsing completed in {}",
            format_duration(parsing_duration)
        );
        println!("DEBUG: Parsed AST: {ast}");
    }

    // We'll walk the AST to build a set of all terms. We track excluded as well for reference.
    let term_collection_start = Instant::now();

    if debug_mode {
        println!("DEBUG: Starting term collection from AST");
    }

    let mut all_terms = Vec::new();
    let mut excluded_terms = HashSet::new();
    collect_all_terms(&ast, &mut all_terms, &mut excluded_terms);

    // Remove duplicates from all_terms
    all_terms.sort();
    all_terms.dedup();

    let term_collection_duration = term_collection_start.elapsed();

    if debug_mode {
        println!(
            "DEBUG: Term collection completed in {}",
            format_duration(term_collection_duration)
        );
        println!("DEBUG: Collected {} unique terms", all_terms.len());
        println!("DEBUG: Collected {} excluded terms", excluded_terms.len());
    }

    // Build term index map
    let index_building_start = Instant::now();

    let mut term_indices = HashMap::new();
    for (i, term) in all_terms.iter().enumerate() {
        term_indices.insert(term.clone(), i);
    }

    let index_building_duration = index_building_start.elapsed();

    if debug_mode {
        println!(
            "DEBUG: Term index building completed in {}",
            format_duration(index_building_duration)
        );
    }

    let total_duration = start_time.elapsed();
    if debug_mode {
        println!(
            "DEBUG: Query plan creation completed in {}",
            format_duration(total_duration)
        );
    }

    // Collect required terms for optimization
    let mut required_terms = HashSet::new();
    collect_required_terms(&ast, &mut required_terms);

    // Determine if this is a simple query for optimization
    let is_simple_query = match &ast {
        elastic_query::Expr::Term { excluded, .. } => !excluded && all_terms.len() == 1,
        _ => false,
    };

    // PHASE 3C OPTIMIZATION: Pre-compute AST metadata
    let has_required_anywhere = ast.has_required_term();
    let has_only_excluded_terms = ast.is_only_excluded_terms();

    // Pre-compute required term indices
    let required_terms_indices: HashSet<usize> = required_terms
        .iter()
        .filter_map(|term| term_indices.get(term).cloned())
        .collect();

    // PHASE 5 OPTIMIZATION: Pre-compute special case terms once
    let mut special_case_indices = HashSet::new();
    let mut special_case_terms_lower = HashMap::new();
    for (term, &idx) in &term_indices {
        if tokenization::is_special_case(term) {
            special_case_indices.insert(idx);
            special_case_terms_lower.insert(idx, term.to_lowercase());
        }
    }

    // Create evaluation cache with reasonable capacity
    let evaluation_cache = Arc::new(Mutex::new(LruCache::new(NonZeroUsize::new(1000).unwrap())));

    Ok(QueryPlan {
        ast,
        term_indices,
        excluded_terms,
        exact,
        is_simple_query,
        required_terms,
        has_required_anywhere,
        required_terms_indices,
        has_only_excluded_terms,
        evaluation_cache,
        is_universal_query: false,
        special_case_indices,
        special_case_terms_lower,
    })
}

/// Collect required terms from the AST for optimization
fn collect_required_terms(expr: &elastic_query::Expr, required_terms: &mut HashSet<String>) {
    match expr {
        elastic_query::Expr::Term {
            keywords,
            required,
            excluded,
            ..
        } => {
            if *required && !*excluded {
                for keyword in keywords {
                    required_terms.insert(keyword.clone());
                }
            }
        }
        elastic_query::Expr::And(left, right) => {
            collect_required_terms(left, required_terms);
            collect_required_terms(right, required_terms);
        }
        elastic_query::Expr::Or(_, _) => {
            // For OR expressions, we can't guarantee any term is required
            // so we don't collect anything
        }
    }
}

/// Recursively update the AST to mark all terms as exact
fn update_ast_exact(expr: &mut elastic_query::Expr) {
    match expr {
        elastic_query::Expr::Term { exact, .. } => {
            // Set exact to true for all terms
            *exact = true;
        }
        elastic_query::Expr::And(left, right) => {
            update_ast_exact(left);
            update_ast_exact(right);
        }
        elastic_query::Expr::Or(left, right) => {
            update_ast_exact(left);
            update_ast_exact(right);
        }
    }
}

/// Helper function to check if the AST represents an exact search
fn is_exact_search(expr: &elastic_query::Expr) -> bool {
    match expr {
        elastic_query::Expr::Term { exact, .. } => *exact,
        elastic_query::Expr::And(left, right) => is_exact_search(left) && is_exact_search(right),
        elastic_query::Expr::Or(left, right) => is_exact_search(left) && is_exact_search(right),
    }
}

/// Recursively collect all terms from the AST, storing them in `all_terms`.
/// Also track excluded terms in `excluded`.
fn collect_all_terms(
    expr: &elastic_query::Expr,
    all_terms: &mut Vec<String>,
    excluded: &mut HashSet<String>,
) {
    let debug_mode = std::env::var("DEBUG").unwrap_or_default() == "1";

    if debug_mode {
        println!("DEBUG: Collecting terms from expression: {expr:?}");
    }

    match expr {
        elastic_query::Expr::Term {
            lowercase_keywords,
            field: _,
            excluded: is_excluded,
            ..
        } => {
            // Use pre-computed lowercase_keywords instead of re-converting
            all_terms.extend(lowercase_keywords.iter().cloned());

            if debug_mode {
                println!(
                    "DEBUG: Collected keywords '{lowercase_keywords:?}', excluded={is_excluded}"
                );
            }

            if *is_excluded {
                for keyword in lowercase_keywords {
                    if debug_mode {
                        println!("DEBUG: Adding '{keyword}' to excluded terms set");
                    }

                    // Use pre-computed lowercase keywords
                    excluded.insert(keyword.clone());
                }
            }
        }
        elastic_query::Expr::And(left, right) => {
            if debug_mode {
                println!("DEBUG: Processing AND expression for term collection");
            }

            // Check if the right side is an excluded term
            if let elastic_query::Expr::Term {
                lowercase_keywords,
                excluded: true,
                ..
            } = &**right
            {
                for keyword in lowercase_keywords {
                    if debug_mode {
                        println!("DEBUG: Adding excluded term '{keyword}' from AND expression");
                    }
                    excluded.insert(keyword.clone());
                }
            }

            collect_all_terms(left, all_terms, excluded);
            collect_all_terms(right, all_terms, excluded);
        }
        elastic_query::Expr::Or(left, right) => {
            if debug_mode {
                println!("DEBUG: Processing OR expression for term collection");
            }
            collect_all_terms(left, all_terms, excluded);
            collect_all_terms(right, all_terms, excluded);
        }
    }

    if debug_mode {
        println!("DEBUG: Current all_terms: {all_terms:?}");
        println!("DEBUG: Current excluded terms: {excluded:?}");
    }
}

/// Build a combined regex pattern from a list of terms
/// This creates a single pattern that matches any of the terms using case-insensitive matching
/// without word boundaries for more flexible matching
pub fn build_combined_pattern(terms: &[String]) -> String {
    let debug_mode = std::env::var("DEBUG").unwrap_or_default() == "1";
    let start_time = Instant::now();

    if debug_mode {
        println!("DEBUG: Building combined pattern for {} terms", terms.len());
    }

    // Limit the number of terms to prevent regex size explosion
    const MAX_TERMS_IN_PATTERN: usize = 1000;
    let limited_terms = if terms.len() > MAX_TERMS_IN_PATTERN {
        if debug_mode {
            println!(
                "DEBUG: Limiting pattern to first {} terms (was {})",
                MAX_TERMS_IN_PATTERN,
                terms.len()
            );
        }
        &terms[..MAX_TERMS_IN_PATTERN]
    } else {
        terms
    };

    // Escape special characters in each term
    let escaped_terms = limited_terms
        .iter()
        .map(|t| regex_escape(t))
        .collect::<Vec<_>>();

    // Join terms with | operator and add case-insensitive flag without word boundaries
    let pattern = format!("(?i)({terms})", terms = escaped_terms.join("|"));

    if debug_mode {
        let duration = start_time.elapsed();
        println!(
            "DEBUG: Combined pattern built in {} with {} terms: {}",
            format_duration(duration),
            limited_terms.len(),
            if pattern.len() > 200 {
                format!("{}...", &pattern[..200])
            } else {
                pattern.clone()
            }
        );
    }

    pattern
}

/// Generate regex patterns that respect the AST's logical structure.
/// This creates a single combined pattern for all terms, regardless of whether they're
/// required, optional, or negative.
pub fn create_structured_patterns(plan: &QueryPlan) -> Vec<(String, HashSet<usize>)> {
    let debug_mode = std::env::var("DEBUG").unwrap_or_default() == "1";
    let start_time = Instant::now();

    if debug_mode {
        println!("DEBUG: Starting structured pattern creation");
        println!("DEBUG: Using combined pattern mode");
    }

    let mut results = Vec::new();
    const MAX_PATTERNS: usize = 5000; // Limit total patterns to prevent regex size explosion

    if debug_mode {
        println!("DEBUG: Creating structured patterns with AST awareness");
        println!("DEBUG: AST: {ast:?}", ast = plan.ast);
        println!(
            "DEBUG: Excluded terms: {excluded_terms:?}",
            excluded_terms = plan.excluded_terms
        );
    }

    // Extract ALL terms from the query plan (including excluded ones)
    // Excluded terms need to be found during search so they can be properly excluded during evaluation
    let terms: Vec<String> = plan.term_indices.keys().cloned().collect();

    if !terms.is_empty() {
        let combined_pattern = build_combined_pattern(&terms);

        // Create a HashSet with indices of non-excluded terms
        let all_indices: HashSet<usize> = terms
            .iter()
            .filter_map(|term| plan.term_indices.get(term).cloned())
            .collect();

        if debug_mode {
            println!("DEBUG: Created combined pattern for all terms: '{combined_pattern}'");
            println!("DEBUG: Combined pattern includes indices: {all_indices:?}");
        }

        results.push((combined_pattern, all_indices));

        // Continue to generate individual patterns instead of returning early
    }

    // Define the recursive helper function *before* calling it
    fn collect_patterns(
        expr: &elastic_query::Expr,
        plan: &QueryPlan,
        results: &mut Vec<(String, HashSet<usize>)>,
        debug_mode: bool,
    ) {
        match expr {
            elastic_query::Expr::Term {
                keywords,
                field: _,
                excluded,
                exact,
                ..
            } => {
                // FIXED: Don't skip pattern generation for excluded terms
                // Excluded terms need to be found during search so they can be properly excluded during evaluation
                if debug_mode && *excluded {
                    println!(
                        "DEBUG: Generating patterns for excluded term (will be filtered during evaluation): '{keywords:?}'"
                    );
                }

                // Process each keyword
                for keyword in keywords {
                    // Note: We still generate patterns for excluded terms so they can be found and then filtered out
                    if debug_mode && plan.excluded_terms.contains(keyword) {
                        println!(
                            "DEBUG: Generating pattern for globally excluded keyword (will be filtered during evaluation): '{keyword}'"
                        );
                    }
                    // The original check `if *excluded` (line 352) already handles terms explicitly marked with `-`
                    // No need for an additional check here for `*excluded` as the outer check handles it.

                    // Find the keyword's index in term_indices
                    if let Some(&idx) = plan.term_indices.get(keyword) {
                        let base_pattern = regex_escape(keyword);

                        // For exact terms, use stricter matching
                        let pattern = if *exact {
                            base_pattern.to_string()
                        } else {
                            format!("({base_pattern})")
                        };

                        if debug_mode {
                            println!("DEBUG: Created pattern for keyword '{keyword}': '{pattern}'");
                        }

                        results.push((pattern, HashSet::from([idx])));

                        // Only tokenize if not exact
                        if !*exact {
                            // Generate patterns for each token of the term to match AST tokenization
                            let tokens = crate::search::tokenization::tokenize_and_stem(keyword);

                            if debug_mode && tokens.len() > 1 {
                                println!("DEBUG: Term '{keyword}' tokenized into: {tokens:?}");
                            }

                            // Generate a pattern for each token with the same term index
                            for token in tokens {
                                let token_pattern = regex_escape(&token);
                                let pattern = format!("({token_pattern})");

                                if debug_mode {
                                    println!(
                                            "DEBUG: Created pattern for token '{token}' from term '{keyword}': '{pattern}'"
                                        );
                                }

                                results.push((pattern, HashSet::from([idx])));
                            }
                        } else if debug_mode {
                            println!("DEBUG: Skipping tokenization for exact term '{keyword}'");
                        }
                    }
                }
            }
            elastic_query::Expr::And(left, right) => {
                // For AND, collect patterns from both sides independently
                if debug_mode {
                    println!("DEBUG: Processing AND expression");
                }
                collect_patterns(left, plan, results, debug_mode);
                collect_patterns(right, plan, results, debug_mode);
            }
            elastic_query::Expr::Or(left, right) => {
                if debug_mode {
                    println!("DEBUG: Processing OR expression");
                }

                // For OR, just collect patterns from both sides independently
                // Don't create complex nested patterns that can explode in size
                collect_patterns(left, plan, results, debug_mode);
                collect_patterns(right, plan, results, debug_mode);
            }
        }
    }
    // Removed extra closing brace after collect_patterns definition

    // Always call the recursive pattern collection logic
    // Removed unused variable 'standard_start'
    if debug_mode {
        println!("DEBUG: Using recursive pattern generation via collect_patterns");
    }
    collect_patterns(&plan.ast, plan, &mut results, debug_mode);

    // Additional pass for compound words
    let compound_start = Instant::now();

    if debug_mode {
        println!("DEBUG: Starting compound word pattern generation");
    }

    let mut compound_patterns = Vec::new();

    // Process all terms from the term_indices map
    for (keyword, &idx) in &plan.term_indices {
        // Check if the original keyword itself is excluded before processing for compound parts
        if plan.excluded_terms.contains(keyword) {
            if debug_mode {
                println!("DEBUG: Skipping compound processing for excluded keyword: '{keyword}'");
            }
            continue; // Skip this keyword entirely
        }

        // Process compound words - either camelCase or those in the vocabulary
        // Skip compound word processing if exact search is enabled
        if keyword.len() > 3 && !is_exact_search(&plan.ast) {
            // Check if it's a camelCase word or a known compound word from vocabulary
            let camel_parts = crate::search::tokenization::split_camel_case(keyword);
            let compound_parts = if camel_parts.len() <= 1 {
                // Not a camelCase word, check if it's in vocabulary
                // VOCABULARY CACHE OPTIMIZATION: Use cached compound word splitting for filtering
                crate::search::tokenization::split_compound_word_for_filtering(keyword)
            } else {
                camel_parts
            };

            if compound_parts.len() > 1 {
                if debug_mode {
                    println!("DEBUG: Processing compound word: '{keyword}'");
                }

                for part in compound_parts {
                    // Check if the part itself is excluded before adding its pattern
                    if part.len() >= 3 && !plan.excluded_terms.contains(&part) {
                        let part_pattern = regex_escape(&part);
                        let pattern = format!("({part_pattern})");

                        if debug_mode {
                            println!(
                                "DEBUG: Adding compound part pattern: '{pattern}' from '{part}'"
                            );
                        }
                        compound_patterns.push((pattern, HashSet::from([idx])));
                    } else if debug_mode && plan.excluded_terms.contains(&part) {
                        println!(
                            "DEBUG: Skipping excluded compound part: '{part}' from keyword '{keyword}'"
                        );
                    } else if debug_mode {
                        println!(
                            "DEBUG: Skipping short compound part: '{part}' from keyword '{keyword}'"
                        );
                    }
                }
            }
        } else if debug_mode && is_exact_search(&plan.ast) {
            println!("DEBUG: Skipping compound word processing for exact search term: '{keyword}'");
        }
    }

    // Store the length before moving compound_patterns
    let compound_patterns_len = compound_patterns.len();

    // Add compound patterns after AST-based patterns
    results.extend(compound_patterns);

    let compound_duration = compound_start.elapsed();

    if debug_mode {
        println!(
            "DEBUG: Compound word pattern generation completed in {} - Generated {} patterns",
            format_duration(compound_duration),
            compound_patterns_len
        );
    }

    // Removed misplaced debug logging block and extra closing brace from old 'else' structure

    // Deduplicate patterns by combining those with the same regex but different indices
    // Also deduplicate patterns that match the same terms
    let dedup_start = Instant::now();

    if debug_mode {
        println!("DEBUG: Starting pattern deduplication");
    }

    // First, deduplicate by exact pattern match using BTreeMap for deterministic iteration
    let mut pattern_map: std::collections::BTreeMap<String, HashSet<usize>> =
        std::collections::BTreeMap::new();

    for (pattern, indices) in results {
        pattern_map
            .entry(pattern)
            .and_modify(|existing_indices| existing_indices.extend(indices.iter().cloned()))
            .or_insert(indices);
    }

    // Then, deduplicate patterns that match the same term using BTreeMap for deterministic iteration
    let mut term_patterns: std::collections::BTreeMap<String, Vec<(String, HashSet<usize>)>> =
        std::collections::BTreeMap::new();

    // Group patterns by the terms they match
    for (pattern, indices) in pattern_map.iter() {
        // Create a key based on the sorted indices
        let mut idx_vec: Vec<usize> = indices.iter().cloned().collect();
        idx_vec.sort();
        let key = idx_vec
            .iter()
            .map(|i| i.to_string())
            .collect::<Vec<_>>()
            .join(",");

        term_patterns
            .entry(key)
            .or_default()
            .push((pattern.clone(), indices.clone()));
    }

    // Keep only the most specific patterns for each term group, sorted deterministically
    let mut deduplicated_results = Vec::new();

    for (_, mut patterns) in term_patterns {
        if patterns.len() <= 2 {
            // If there are 1 or 2 patterns, keep them all
            deduplicated_results.extend(patterns);
        } else {
            // Sort patterns by specificity: longer patterns first, then lexicographic
            patterns.sort_by(|a, b| b.0.len().cmp(&a.0.len()).then_with(|| a.0.cmp(&b.0)));

            // Keep the 2 most specific patterns
            deduplicated_results.extend(patterns.into_iter().take(2));
        }
    }

    let dedup_duration = dedup_start.elapsed();

    if debug_mode {
        println!(
            "DEBUG: Pattern deduplication completed in {} - Final pattern count: {}",
            format_duration(dedup_duration),
            deduplicated_results.len()
        );
        for (pattern, indices) in &deduplicated_results {
            println!("DEBUG: Pattern: '{pattern}', Indices: {indices:?}");
        }
    }

    // Sort the final results deterministically before applying limits
    deduplicated_results.sort_by(|a, b| {
        // Sort by smallest term index first, then by pattern length (longer first), then lexicographic
        let min_index_a = a.1.iter().min().unwrap_or(&usize::MAX);
        let min_index_b = b.1.iter().min().unwrap_or(&usize::MAX);

        min_index_a
            .cmp(min_index_b)
            .then_with(|| b.0.len().cmp(&a.0.len()))
            .then_with(|| a.0.cmp(&b.0))
    });

    // Apply pattern limit to prevent regex size explosion
    let limited_results = if deduplicated_results.len() > MAX_PATTERNS {
        if debug_mode {
            println!(
                "DEBUG: Limiting patterns to {} (was {})",
                MAX_PATTERNS,
                deduplicated_results.len()
            );
        }
        deduplicated_results
            .into_iter()
            .take(MAX_PATTERNS)
            .collect()
    } else {
        deduplicated_results
    };

    let total_duration = start_time.elapsed();

    if debug_mode {
        println!(
            "DEBUG: Total structured pattern creation completed in {} with {} patterns",
            format_duration(total_duration),
            limited_results.len()
        );
    }

    limited_results
} // Re-added function closing brace

/// Create a query plan from an already parsed AST
pub fn create_query_plan_from_ast(
    ast: elastic_query::Expr,
    exact: bool,
) -> Result<QueryPlan, elastic_query::ParseError> {
    let debug_mode = std::env::var("DEBUG").unwrap_or_default() == "1";
    let start_time = Instant::now();

    if debug_mode {
        println!("DEBUG: Creating query plan from existing AST");
    }

    // Update AST for exact search if needed
    let mut final_ast = ast;
    if exact {
        update_ast_exact(&mut final_ast);
    }

    // Extract terms from the AST
    let mut all_terms = Vec::new();
    let mut excluded_terms = HashSet::new();
    collect_all_terms(&final_ast, &mut all_terms, &mut excluded_terms);

    // Remove duplicates from all_terms
    all_terms.sort();
    all_terms.dedup();

    // Build term index map
    let mut term_indices = HashMap::new();
    for (i, term) in all_terms.iter().enumerate() {
        term_indices.insert(term.clone(), i);
    }

    // Collect required terms for optimization
    let mut required_terms = HashSet::new();
    collect_required_terms(&final_ast, &mut required_terms);

    // Determine if this is a simple query for optimization
    let is_simple_query = match &final_ast {
        elastic_query::Expr::Term { excluded, .. } => !excluded && all_terms.len() == 1,
        _ => false,
    };

    // Pre-compute AST metadata
    let has_required_anywhere = final_ast.has_required_term();
    let has_only_excluded_terms = final_ast.is_only_excluded_terms();

    // Pre-compute required term indices
    let required_terms_indices: HashSet<usize> = required_terms
        .iter()
        .filter_map(|term| term_indices.get(term).cloned())
        .collect();

    // PHASE 5 OPTIMIZATION: Pre-compute special case terms once
    let mut special_case_indices = HashSet::new();
    let mut special_case_terms_lower = HashMap::new();
    for (term, &idx) in &term_indices {
        if tokenization::is_special_case(term) {
            special_case_indices.insert(idx);
            special_case_terms_lower.insert(idx, term.to_lowercase());
        }
    }

    // Create evaluation cache
    let evaluation_cache = Arc::new(Mutex::new(LruCache::new(NonZeroUsize::new(1000).unwrap())));

    let total_duration = start_time.elapsed();
    if debug_mode {
        println!(
            "DEBUG: Query plan from AST completed in {}",
            format_duration(total_duration)
        );
    }

    Ok(QueryPlan {
        ast: final_ast,
        term_indices,
        excluded_terms,
        exact,
        is_simple_query,
        required_terms,
        has_required_anywhere,
        required_terms_indices,
        has_only_excluded_terms,
        evaluation_cache,
        is_universal_query: false,
        special_case_indices,
        special_case_terms_lower,
    })
}

/// Create a universal query plan that matches everything (used when all terms are filters)
pub fn create_universal_query_plan() -> QueryPlan {
    // Create a simple term that will match anything in the content
    // Use common characters that will appear in almost any file
    let keywords = vec![".".to_string()]; // Match any single character - will match almost everything
    let universal_ast = elastic_query::Expr::Term {
        lowercase_keywords: keywords.iter().map(|k| k.to_lowercase()).collect(),
        keywords,
        field: None,
        required: false,
        excluded: false,
        exact: false,
    };

    let mut term_indices = HashMap::new();
    term_indices.insert(".".to_string(), 0);

    QueryPlan {
        ast: universal_ast,
        term_indices,
        excluded_terms: HashSet::new(),
        exact: false,
        is_simple_query: true,
        required_terms: HashSet::new(),
        has_required_anywhere: false,
        required_terms_indices: HashSet::new(),
        has_only_excluded_terms: false,
        evaluation_cache: Arc::new(Mutex::new(LruCache::new(NonZeroUsize::new(1000).unwrap()))),
        is_universal_query: true, // This is a universal query that should match all content
        special_case_indices: HashSet::new(),
        special_case_terms_lower: HashMap::new(),
    }
}
